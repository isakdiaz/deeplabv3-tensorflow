{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "treeseg_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMsX8O8Mu4bn40phqvtBDmk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isakdiaz/deeplabv3-tree-segmentation/blob/master/treeseg_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iMrbBfGWVZG"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeQfSOzhWvLi"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pprint\n",
        "import json\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo2LzWFZXnU7"
      },
      "source": [
        "# Check GPU or TPU Enabled"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0mu4x81W3YD"
      },
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('✅ Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  print(\"❌ No TPU Found! At the top click Runtime -> Change Runtime Type -> Hardware Accelerator -> TPU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8nqnws_W5cA"
      },
      "source": [
        "# Check if running on GPU\n",
        "import tensorflow as tf\n",
        "gpu_location = tf.test.gpu_device_name()\n",
        "if gpu_location:\n",
        "  print(\"✅ GPU found at \" + gpu_location)\n",
        "else:\n",
        "  print(\"❌ No GPU Found!. At the top click Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWGE3VH3Xk_r"
      },
      "source": [
        "# Clone The Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e8fz_pyW6hU"
      },
      "source": [
        "!git clone https://github.com/isakdiaz/deeplabv3-tree-segmentation.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdVbyMzYXiZK"
      },
      "source": [
        "!cd & ls deeplabv3-tree-segmentation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGpTvfZHZ0v6"
      },
      "source": [
        "## Confirm that augmented dataset has already been generated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4qKWxsvXxP8"
      },
      "source": [
        "# try:\n",
        "#   num_train =  !cd & ls deeplabv3-tree-segmentation/data_augmented/train/image | wc -l\n",
        "# num_test = !cd & ls deeplabv3-tree-segmentation/data_augmented/test/image | wc -l\n",
        "\n",
        "# if num_train:\n",
        "#   print(f\"There are {num_train[0]} images in the training set and {num_test[0]} in the test set.\")\n",
        "# else:\n",
        "#   print(\"Data has not been augmented. Follow the steps below to generate data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rQKee27XmRG"
      },
      "source": [
        "# Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6oxZ79jZKa7"
      },
      "source": [
        "!pip install -q -U albumentations\n",
        "!echo \"$(pip freeze | grep albumentations) is successfully installed\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZBxLB63Xoxn"
      },
      "source": [
        "!cd deeplabv3-tree-segmentation && python prep_renders.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d019720"
      },
      "source": [
        "!cd deeplabv3-tree-segmentation && python data.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnsuE0J0cJoI"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOnpHspMcLe-"
      },
      "source": [
        "## Method 1 - Run the PY File. (currently not working, batch size is too big)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swYoxciGZgAR"
      },
      "source": [
        "!cd deeplabv3-tree-segmentation && python train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0REEYY4gaOyH"
      },
      "source": [
        "## Method 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV04ypYoX4gS"
      },
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Recall, Precision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVTW3ybtaQsu"
      },
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, Reshape, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import tensorflow as tf\n",
        "\n",
        "def SqueezeAndExcite(inputs, ratio=8):\n",
        "    init = inputs\n",
        "    filters = init.shape[-1]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    x = init * se\n",
        "    return x\n",
        "\n",
        "def ASPP(inputs):\n",
        "    \"\"\" Image Pooling \"\"\"\n",
        "    shape = inputs.shape\n",
        "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(inputs)\n",
        "    y1 = Conv2D(256, 1, padding=\"same\", use_bias=False)(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation(\"relu\")(y1)\n",
        "    y1 = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y1)\n",
        "\n",
        "    \"\"\" 1x1 conv \"\"\"\n",
        "    y2 = Conv2D(256, 1, padding=\"same\", use_bias=False)(inputs)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    y2 = Activation(\"relu\")(y2)\n",
        "\n",
        "    \"\"\" 3x3 conv rate=6 \"\"\"\n",
        "    y3 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=6)(inputs)\n",
        "    y3 = BatchNormalization()(y3)\n",
        "    y3 = Activation(\"relu\")(y3)\n",
        "\n",
        "    \"\"\" 3x3 conv rate=12 \"\"\"\n",
        "    y4 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=12)(inputs)\n",
        "    y4 = BatchNormalization()(y4)\n",
        "    y4 = Activation(\"relu\")(y4)\n",
        "\n",
        "    \"\"\" 3x3 conv rate=18 \"\"\"\n",
        "    y5 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=18)(inputs)\n",
        "    y5 = BatchNormalization()(y5)\n",
        "    y5 = Activation(\"relu\")(y5)\n",
        "\n",
        "    y = Concatenate()([y1, y2, y3, y4, y5])\n",
        "    y = Conv2D(256, 1, padding=\"same\", use_bias=False)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation(\"relu\")(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "def deeplabv3_plus(shape):\n",
        "    \"\"\" Input \"\"\"\n",
        "    inputs = Input(shape)\n",
        "\n",
        "    \"\"\" Encoder \"\"\"\n",
        "    encoder = ResNet50(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n",
        "\n",
        "    image_features = encoder.get_layer(\"conv4_block6_out\").output\n",
        "    x_a = ASPP(image_features)\n",
        "    x_a = UpSampling2D((4, 4), interpolation=\"bilinear\")(x_a)\n",
        "\n",
        "    x_b = encoder.get_layer(\"conv2_block2_out\").output\n",
        "    x_b = Conv2D(filters=48, kernel_size=1, padding='same', use_bias=False)(x_b)\n",
        "    x_b = BatchNormalization()(x_b)\n",
        "    x_b = Activation('relu')(x_b)\n",
        "\n",
        "    x = Concatenate()([x_a, x_b])\n",
        "    x = SqueezeAndExcite(x)\n",
        "\n",
        "    x = Conv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SqueezeAndExcite(x)\n",
        "\n",
        "    x = UpSampling2D((4, 4), interpolation=\"bilinear\")(x)\n",
        "    x = Conv2D(1, 1)(x)\n",
        "    x = Activation(\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs, x)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_v31Yqyb4ip"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    def f(y_true, y_pred):\n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        union = y_true.sum() + y_pred.sum() - intersection\n",
        "        x = (intersection + 1e-15) / (union + 1e-15)\n",
        "        x = x.astype(np.float32)\n",
        "        return x\n",
        "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
        "\n",
        "smooth = 1e-15\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoWLCfd2b_Ii"
      },
      "source": [
        "\"\"\" Global parameters \"\"\"\n",
        "H = 512\n",
        "W = 512\n",
        "\n",
        "\"\"\" Creating a directory \"\"\"\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def shuffling(x, y):\n",
        "    x, y = shuffle(x, y, random_state=42)\n",
        "    return x, y\n",
        "\n",
        "def load_data(path):\n",
        "    x = sorted(glob(os.path.join(path, \"image\", \"*jpg\")))\n",
        "    y = sorted(glob(os.path.join(path, \"mask\", \"*png\")))\n",
        "    return x, y\n",
        "\n",
        "def read_image(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    return x\n",
        "\n",
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset(X, Y, batch=2):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(10)\n",
        "    return dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVk-1i76cFz9",
        "outputId": "3acbbf0d-7dc8-43a4-e3dc-170d71d9f620"
      },
      "source": [
        "  \"\"\" Seeding \"\"\"\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "\n",
        "  \"\"\" Directory for storing files \"\"\"\n",
        "  create_dir(\"files\")\n",
        "\n",
        "  \"\"\" Hyperparameters \"\"\"\n",
        "  batch_size = 8\n",
        "  lr = 1e-4\n",
        "  num_epochs = 20\n",
        "  model_path = os.path.join(\"files\", \"model.h5\")\n",
        "  csv_path = os.path.join(\"files\", \"data.csv\")\n",
        "\n",
        "  \"\"\" Dataset \"\"\"\n",
        "  dataset_path = \"deeplabv3-tree-segmentation/data_augmented\"\n",
        "  train_path = os.path.join(dataset_path, \"train\")\n",
        "  valid_path = os.path.join(dataset_path, \"test\")\n",
        "\n",
        "  print(train_path)\n",
        "  train_x, train_y = load_data(train_path)\n",
        "  print(len(train_x))\n",
        "  print(len(train_y))\n",
        "\n",
        "  train_x, train_y = shuffling(train_x, train_y)\n",
        "  valid_x, valid_y = load_data(valid_path)\n",
        "\n",
        "  print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
        "  print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
        "\n",
        "  train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n",
        "  valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n",
        "\n",
        "  \"\"\" Model \"\"\"\n",
        "  model = deeplabv3_plus((H, W, 3))\n",
        "  model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=[dice_coef, iou, Recall(), Precision()])\n",
        "\n",
        "  callbacks = [\n",
        "      ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "      ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
        "      CSVLogger(csv_path),\n",
        "      TensorBoard(),\n",
        "      EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False),\n",
        "  ]\n",
        "  \n",
        "  tf.config.list_physical_devices()\n",
        "\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      epochs=num_epochs,\n",
        "      validation_data=valid_dataset,\n",
        "      callbacks=callbacks\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "198/203 [============================>.] - ETA: 4s - loss: 0.1713 - dice_coef: 0.8287 - iou: 0.7288 - recall_3: 0.9421 - precision_3: 0.8115"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoRy5LZcck0I"
      },
      "source": [
        "!ls deeplabv3-tree-segmentation/data_augmented/train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL7gBGgBclmO"
      },
      "source": [
        "!ls -hs files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D54DgsgFhI8g"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RvY3XBQgYSL"
      },
      "source": [
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P1BnZtUhMnI"
      },
      "source": [
        "def save_results(image, mask, y_pred, save_image_path):\n",
        "    ## i - m - yp - yp*i\n",
        "    line = np.ones((H, 10, 3)) * 128\n",
        "\n",
        "    mask = np.expand_dims(mask, axis=-1)    ## (512, 512, 1)\n",
        "    mask = np.concatenate([mask, mask, mask], axis=-1)  ## (512, 512, 3)\n",
        "    mask = mask * 255\n",
        "\n",
        "    y_pred = np.expand_dims(y_pred, axis=-1)    ## (512, 512, 1)\n",
        "    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis=-1)  ## (512, 512, 3)\n",
        "\n",
        "    masked_image = image * y_pred\n",
        "    y_pred = y_pred * 255\n",
        "\n",
        "    cat_images = np.concatenate([image, line, mask, line, y_pred, line, masked_image], axis=1)\n",
        "    cv2.imwrite(save_image_path, cat_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z8NnO6BhX6u"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\"\"\" Directory for storing files \"\"\"\n",
        "create_dir(\"results\")\n",
        "\n",
        "\"\"\" Loading model \"\"\"\n",
        "with CustomObjectScope({'iou': iou, 'dice_coef': dice_coef, 'dice_loss': dice_loss}):\n",
        "    model = tf.keras.models.load_model(\"files/model.h5\")\n",
        "\n",
        "\"\"\" Load the dataset \"\"\"\n",
        "dataset_path = \"deeplabv3-tree-segmentation/data_augmented\"\n",
        "valid_path = os.path.join(dataset_path, \"test\")\n",
        "test_x, test_y = load_data(valid_path)\n",
        "print(f\"Test: {len(test_x)} - {len(test_y)}\")\n",
        "\n",
        "\"\"\" Evaluation and Prediction \"\"\"\n",
        "SCORE = []\n",
        "for x, y in tqdm(zip(test_x, test_y), total=len(test_x)):\n",
        "    \"\"\" Extract the name \"\"\"\n",
        "    name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "    \"\"\" Reading the image \"\"\"\n",
        "    image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "    x = image/255.0\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    \"\"\" Reading the mask \"\"\"\n",
        "    mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
        "    mask = mask/255\n",
        "\n",
        "    \"\"\" Prediction \"\"\"\n",
        "    y_pred = model.predict(x)[0]\n",
        "    y_pred = np.squeeze(y_pred, axis=-1)\n",
        "    y_pred = y_pred > 0.5\n",
        "    y_pred = y_pred.astype(np.int32)\n",
        "\n",
        "    \"\"\" Saving the prediction \"\"\"\n",
        "    save_image_path = f\"results/{name}.png\"\n",
        "    save_results(image, mask, y_pred, save_image_path)\n",
        "\n",
        "    \"\"\" Flatten the array \"\"\"\n",
        "    mask = mask.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "\n",
        "    \"\"\" Calculating the metrics values \"\"\"\n",
        "    acc_value = accuracy_score(mask, y_pred)\n",
        "    f1_value = f1_score(mask, y_pred, labels=[0, 1], average=\"binary\")\n",
        "    jac_value = jaccard_score(mask, y_pred, labels=[0, 1], average=\"binary\")\n",
        "    recall_value = recall_score(mask, y_pred, labels=[0, 1], average=\"binary\")\n",
        "    precision_value = precision_score(mask, y_pred, labels=[0, 1], average=\"binary\")\n",
        "    SCORE.append([name, acc_value, f1_value, jac_value, recall_value, precision_value])\n",
        "\n",
        "\"\"\" Metrics values \"\"\"\n",
        "score = [s[1:]for s in SCORE]\n",
        "score = np.mean(score, axis=0)\n",
        "print(f\"Accuracy: {score[0]:0.5f}\")\n",
        "print(f\"F1: {score[1]:0.5f}\")\n",
        "print(f\"Jaccard: {score[2]:0.5f}\")\n",
        "print(f\"Recall: {score[3]:0.5f}\")\n",
        "print(f\"Precision: {score[4]:0.5f}\")\n",
        "\n",
        "df = pd.DataFrame(SCORE, columns=[\"Image\", \"Accuracy\", \"F1\", \"Jaccard\", \"Recall\", \"Precision\"])\n",
        "df.to_csv(\"files/score.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoBVkitVpPnN"
      },
      "source": [
        "# Save Model to Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thsfPXnZpPIY"
      },
      "source": [
        "from datetime import datetime \n",
        "\n",
        "# Date Time\n",
        "current_time = datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
        "current_date = datetime.now().strftime('%Y-%m-%d')\n",
        "GDRIVE_DIR = \"/content/drive/MyDrive/saved_models/treeseg\" # Save Path for models\n",
        "filepath = os.path.join(GDRIVE_DIR,\"treeseg_\" + current_date + \".h5\")\n",
        "\n",
        "print(f\"Loading file from {filepath}\")\n",
        "\n",
        "\n",
        "model.save(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gykR5au4lr-e"
      },
      "source": [
        "## Display Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHMrshMphT-1"
      },
      "source": [
        "image_filenames = sorted(glob(os.path.join(\"results\", \"*png\")))\n",
        "image_filenames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r813tQfiUCA"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in image_filenames[:4]:\n",
        "    display(Image(filename=imageName))\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkWd0txbkKrT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tho8E2TXkL_9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}