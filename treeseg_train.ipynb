{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "treeseg_train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yo2LzWFZXnU7"
      ],
      "authorship_tag": "ABX9TyPsHMhmWPq9Yp8yWo+ceHjs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isakdiaz/deeplabv3-tree-segmentation/blob/master/treeseg_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iMrbBfGWVZG",
        "outputId": "5bd15e7b-15ff-4696-e060-d3d769bc9780"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeQfSOzhWvLi"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pprint\n",
        "import json\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo2LzWFZXnU7"
      },
      "source": [
        "# Check GPU or TPU Enabled"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0mu4x81W3YD",
        "outputId": "cc1b8e19-1e94-46bf-a20a-6954008ef516"
      },
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('✅ Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  print(\"❌ No TPU Found! At the top click Runtime -> Change Runtime Type -> Hardware Accelerator -> TPU\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "❌ No TPU Found! At the top click Runtime -> Change Runtime Type -> Hardware Accelerator -> TPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8nqnws_W5cA",
        "outputId": "faeca6a6-9068-4569-bc4b-5d2cd80c3ce8"
      },
      "source": [
        "# Check if running on GPU\n",
        "import tensorflow as tf\n",
        "gpu_location = tf.test.gpu_device_name()\n",
        "if gpu_location:\n",
        "  print(\"✅ GPU found at \" + gpu_location)\n",
        "else:\n",
        "  print(\"❌ No GPU Found!. At the top click Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "✅ GPU found at /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWGE3VH3Xk_r"
      },
      "source": [
        "# Clone The Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e8fz_pyW6hU",
        "outputId": "d0d41c91-5252-4bbd-f084-e58d17b392c2"
      },
      "source": [
        "!git clone https://github.com/isakdiaz/deeplabv3-tree-segmentation.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deeplabv3-tree-segmentation'...\n",
            "remote: Enumerating objects: 2025, done.\u001b[K\n",
            "remote: Total 2025 (delta 0), reused 0 (delta 0), pack-reused 2025\u001b[K\n",
            "Receiving objects: 100% (2025/2025), 497.61 MiB | 10.08 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdVbyMzYXiZK",
        "outputId": "b68c2255-bcaa-4e2b-9479-0216738657d6"
      },
      "source": [
        "!cd & ls deeplabv3-tree-segmentation"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\tdata.py  metrics.py  predict.py       Readme.md\n",
            "data_augmented\teval.py  model.py    prep_renders.py  train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGpTvfZHZ0v6"
      },
      "source": [
        "## Confirm that augmented dataset has already been generated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4qKWxsvXxP8",
        "outputId": "1116544a-858f-4d06-98f9-be250ef96323"
      },
      "source": [
        "num_train = !cd & ls deeplabv3-tree-segmentation/data_augmented/train/image | wc -l\n",
        "num_test = !cd & ls deeplabv3-tree-segmentation/data_augmented/test/image | wc -l\n",
        "print(f\"There are {num_imgs[0]} images in the training set and {num_test[0]} in the test set.\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 540 images in the training set and 10 in the test set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0REEYY4gaOyH"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV04ypYoX4gS"
      },
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Recall, Precision"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVTW3ybtaQsu"
      },
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, Reshape, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import tensorflow as tf\n",
        "\n",
        "def SqueezeAndExcite(inputs, ratio=8):\n",
        "    init = inputs\n",
        "    filters = init.shape[-1]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    x = init * se\n",
        "    return x\n",
        "\n",
        "def ASPP(inputs):\n",
        "    \"\"\" Image Pooling \"\"\"\n",
        "    shape = inputs.shape\n",
        "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(inputs)\n",
        "    y1 = Conv2D(256, 1, padding=\"same\", use_bias=False)(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation(\"relu\")(y1)\n",
        "    y1 = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y1)\n",
        "\n",
        "    \"\"\" 1x1 conv \"\"\"\n",
        "    y2 = Conv2D(256, 1, padding=\"same\", use_bias=False)(inputs)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    y2 = Activation(\"relu\")(y2)\n",
        "\n",
        "    \"\"\" 3x3 conv rate=6 \"\"\"\n",
        "    y3 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=6)(inputs)\n",
        "    y3 = BatchNormalization()(y3)\n",
        "    y3 = Activation(\"relu\")(y3)\n",
        "\n",
        "    \"\"\" 3x3 conv rate=12 \"\"\"\n",
        "    y4 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=12)(inputs)\n",
        "    y4 = BatchNormalization()(y4)\n",
        "    y4 = Activation(\"relu\")(y4)\n",
        "\n",
        "    \"\"\" 3x3 conv rate=18 \"\"\"\n",
        "    y5 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=18)(inputs)\n",
        "    y5 = BatchNormalization()(y5)\n",
        "    y5 = Activation(\"relu\")(y5)\n",
        "\n",
        "    y = Concatenate()([y1, y2, y3, y4, y5])\n",
        "    y = Conv2D(256, 1, padding=\"same\", use_bias=False)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation(\"relu\")(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "def deeplabv3_plus(shape):\n",
        "    \"\"\" Input \"\"\"\n",
        "    inputs = Input(shape)\n",
        "\n",
        "    \"\"\" Encoder \"\"\"\n",
        "    encoder = ResNet50(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n",
        "\n",
        "    image_features = encoder.get_layer(\"conv4_block6_out\").output\n",
        "    x_a = ASPP(image_features)\n",
        "    x_a = UpSampling2D((4, 4), interpolation=\"bilinear\")(x_a)\n",
        "\n",
        "    x_b = encoder.get_layer(\"conv2_block2_out\").output\n",
        "    x_b = Conv2D(filters=48, kernel_size=1, padding='same', use_bias=False)(x_b)\n",
        "    x_b = BatchNormalization()(x_b)\n",
        "    x_b = Activation('relu')(x_b)\n",
        "\n",
        "    x = Concatenate()([x_a, x_b])\n",
        "    x = SqueezeAndExcite(x)\n",
        "\n",
        "    x = Conv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SqueezeAndExcite(x)\n",
        "\n",
        "    x = UpSampling2D((4, 4), interpolation=\"bilinear\")(x)\n",
        "    x = Conv2D(1, 1)(x)\n",
        "    x = Activation(\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs, x)\n",
        "    return model"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_v31Yqyb4ip"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    def f(y_true, y_pred):\n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        union = y_true.sum() + y_pred.sum() - intersection\n",
        "        x = (intersection + 1e-15) / (union + 1e-15)\n",
        "        x = x.astype(np.float32)\n",
        "        return x\n",
        "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
        "\n",
        "smooth = 1e-15\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoWLCfd2b_Ii"
      },
      "source": [
        "\"\"\" Global parameters \"\"\"\n",
        "H = 512\n",
        "W = 512\n",
        "\n",
        "\"\"\" Creating a directory \"\"\"\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def shuffling(x, y):\n",
        "    x, y = shuffle(x, y, random_state=42)\n",
        "    return x, y\n",
        "\n",
        "def load_data(path):\n",
        "    x = sorted(glob(os.path.join(path, \"image\", \"*jpg\")))\n",
        "    y = sorted(glob(os.path.join(path, \"mask\", \"*png\")))\n",
        "    return x, y\n",
        "\n",
        "def read_image(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    return x\n",
        "\n",
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset(X, Y, batch=2):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(10)\n",
        "    return dataset\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVk-1i76cFz9",
        "outputId": "1f18efc7-efe0-4d31-c1ea-35195bc50885"
      },
      "source": [
        "  \"\"\" Seeding \"\"\"\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "\n",
        "  \"\"\" Directory for storing files \"\"\"\n",
        "  create_dir(\"files\")\n",
        "\n",
        "  \"\"\" Hyperparameters \"\"\"\n",
        "  batch_size = 4\n",
        "  lr = 1e-4\n",
        "  num_epochs = 20\n",
        "  model_path = os.path.join(\"files\", \"model.h5\")\n",
        "  csv_path = os.path.join(\"files\", \"data.csv\")\n",
        "\n",
        "  \"\"\" Dataset \"\"\"\n",
        "  dataset_path = \"deeplabv3-tree-segmentation/data_augmented\"\n",
        "  train_path = os.path.join(dataset_path, \"train\")\n",
        "  valid_path = os.path.join(dataset_path, \"test\")\n",
        "\n",
        "  print(train_path)\n",
        "  train_x, train_y = load_data(train_path)\n",
        "  print(len(train_x))\n",
        "  print(len(train_y))\n",
        "\n",
        "  train_x, train_y = shuffling(train_x, train_y)\n",
        "  valid_x, valid_y = load_data(valid_path)\n",
        "\n",
        "  print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
        "  print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
        "\n",
        "  train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n",
        "  valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n",
        "\n",
        "  \"\"\" Model \"\"\"\n",
        "  model = deeplabv3_plus((H, W, 3))\n",
        "  model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=[dice_coef, iou, Recall(), Precision()])\n",
        "\n",
        "  callbacks = [\n",
        "      ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "      ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
        "      CSVLogger(csv_path),\n",
        "      TensorBoard(),\n",
        "      EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False),\n",
        "  ]\n",
        "  \n",
        "  tf.config.list_physical_devices()\n",
        "\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      epochs=num_epochs,\n",
        "      validation_data=valid_dataset,\n",
        "      callbacks=callbacks\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deeplabv3-tree-segmentation/data_augmented/train\n",
            "540\n",
            "540\n",
            "Train: 540 - 540\n",
            "Valid: 10 - 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "135/135 [==============================] - 60s 280ms/step - loss: 0.2631 - dice_coef: 0.7369 - iou: 0.6085 - recall_2: 0.9419 - precision_2: 0.7075 - val_loss: 0.8944 - val_dice_coef: 0.1034 - val_iou: 0.0545 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.89436, saving model to files/model.h5\n",
            "Epoch 2/20\n",
            " 41/135 [========>.....................] - ETA: 23s - loss: 0.1081 - dice_coef: 0.8919 - iou: 0.8069 - recall_2: 0.9529 - precision_2: 0.9181"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoRy5LZcck0I",
        "outputId": "702be79e-cf51-418e-986f-4f4ba661b2e7"
      },
      "source": [
        "!ls deeplabv3-tree-segmentation/data_augmented/train"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image  mask\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL7gBGgBclmO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}